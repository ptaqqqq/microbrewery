{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/.conda/envs/MCGLH/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import datasets\n",
    "import torch\n",
    "import gc\n",
    "import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c984832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer\n",
      "model\n",
      "student\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print('tokenizer')\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained('speakleash/Bielik-1.5B-v3.0-Instruct')\n",
    "print('model')\n",
    "teacher = AutoModelForCausalLM.from_pretrained('speakleash/Bielik-1.5B-v3.0-Instruct-FP8-Dynamic').to(device)\n",
    "print('student')\n",
    "student_tokenizer = AutoTokenizer.from_pretrained('sdadas/polish-gpt2-small')\n",
    "student = AutoModelForCausalLM.from_pretrained('sdadas/polish-gpt2-small').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa30014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['instruction', 'output'])\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('Igorrr0/polish-qa-general').remove_columns(['input'])\n",
    "print(dataset['train'][0].keys())\n",
    "\n",
    "def bielik_prompt(prompt):\n",
    "    return f\"<s><|im_start|> user\\n{prompt}<|im_end|> \\n<|im_start|> assistant\\n\"\n",
    "\n",
    "\n",
    "def question_to_prompt(question, answer=\"\", bielik=False):\n",
    "    if bielik:\n",
    "        return bielik_prompt(question) + answer + (\"\\n<|im_end|><s>\" if answer != \"\" else \"\")\n",
    "    return f\"user\\n{question}\\n\\nassistant\\n{answer}\"\n",
    "\n",
    "\n",
    "def batch_to_prompt(batch):\n",
    "    prompts = []\n",
    "    for question, answer in zip(batch['instruction'], batch['output']):\n",
    "        prompts.append(question_to_prompt(question, answer))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56158d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing teacher model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     1,     3, 31887,   310,  2272,    17, 31942,   296,   373,\n",
      "           403, 31887, 31924, 31979, 31924, 31956,     4, 31887, 31887,    17,\n",
      "             3, 31887,   322,  3988, 19681,    17]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33])\n",
      " user\n",
      "Ile to jest 2+2?  \n",
      "  assistant\n",
      "2+2 = 4\n",
      "Testing student model:\n",
      "{'input_ids': tensor([[   22,    15,    22, 22274,   309,   225]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([1, 8])\n",
      "2+2 równa się %.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "def test_model(model, tokenizer, custom_prompt=\"Ile to jest 2+2?\"):\n",
    "    tokens = tokenizer(custom_prompt, return_tensors='pt')\n",
    "    tokens = tokens.to(model.device)\n",
    "    print(tokens)\n",
    "    model.eval()\n",
    "    output = model.generate(**tokens, max_new_tokens=512)\n",
    "    print(output.size())\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(decoded)\n",
    "\n",
    "print('Testing teacher model:')\n",
    "test_model(teacher, teacher_tokenizer, custom_prompt=bielik_prompt(\"Ile to jest 2+2?\"))\n",
    "print('Testing student model:')\n",
    "test_model(student, student_tokenizer, custom_prompt=\"2+2 równa się \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_0\n",
      "Jak mogę pomóc?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_1\n",
      "Cześć!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2\n",
      "Dzień dobry, proszę o informacje.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:4 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_3\n",
      "Czy możesz to powtórzyć?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(x[\u001b[33m'\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m     tokens = teacher_tokenizer(bielik_prompt(x[\u001b[33m'\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m'\u001b[39m]), return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     output = teacher_tokenizer.decode(teacher.generate(**tokens, max_new_tokens=\u001b[32m512\u001b[39m)[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m     distilled_ds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][i][\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m] = output\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(distilled_ds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2598\u001b[39m         input_ids,\n\u001b[32m   2599\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2600\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2601\u001b[39m         generation_config=generation_config,\n\u001b[32m   2602\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2603\u001b[39m         streamer=streamer,\n\u001b[32m   2604\u001b[39m         **model_kwargs,\n\u001b[32m   2605\u001b[39m     )\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    689\u001b[39m     input_ids=input_ids,\n\u001b[32m    690\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    691\u001b[39m     position_ids=position_ids,\n\u001b[32m    692\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    693\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    694\u001b[39m     use_cache=use_cache,\n\u001b[32m    695\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    696\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    697\u001b[39m     cache_position=cache_position,\n\u001b[32m    698\u001b[39m     **kwargs,\n\u001b[32m    699\u001b[39m )\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    451\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    454\u001b[39m     hidden_states,\n\u001b[32m    455\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    456\u001b[39m     position_ids=position_ids,\n\u001b[32m    457\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    458\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    459\u001b[39m     use_cache=use_cache,\n\u001b[32m    460\u001b[39m     cache_position=cache_position,\n\u001b[32m    461\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    462\u001b[39m     **flash_attn_kwargs,\n\u001b[32m    463\u001b[39m )\n\u001b[32m    465\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    309\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    310\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    311\u001b[39m     position_ids=position_ids,\n\u001b[32m    312\u001b[39m     past_key_value=past_key_value,\n\u001b[32m    313\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    314\u001b[39m     use_cache=use_cache,\n\u001b[32m    315\u001b[39m     cache_position=cache_position,\n\u001b[32m    316\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    317\u001b[39m     **kwargs,\n\u001b[32m    318\u001b[39m )\n\u001b[32m    319\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:242\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    240\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    243\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    244\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/compressed_tensors/quantization/lifecycle/forward.py:316\u001b[39m, in \u001b[36mwrap_module_forward_quantized.<locals>.wrapped_forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m compressed = module.quantization_status == QuantizationStatus.COMPRESSED\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheme.input_activations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# prehook should calibrate activations before forward call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     input_ = forward_quantize(module, input_, \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, scheme.input_activations)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheme.weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compressed:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# calibrate and (fake) quantize weights when applicable\u001b[39;00m\n\u001b[32m    320\u001b[39m     unquantized_weight = \u001b[38;5;28mself\u001b[39m.weight.data.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/compressed_tensors/quantization/lifecycle/forward.py:375\u001b[39m, in \u001b[36mforward_quantize\u001b[39m\u001b[34m(module, value, base_name, args)\u001b[39m\n\u001b[32m    371\u001b[39m global_scale = \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_global_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.dynamic \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, DynamicType.LOCAL):\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# dynamic quantization - determine the scale/zp on the fly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     scale, zero_point = compute_dynamic_scales_and_zp(\n\u001b[32m    376\u001b[39m         value=value, args=args, module=module, global_scale=global_scale\n\u001b[32m    377\u001b[39m     )\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    379\u001b[39m     \u001b[38;5;66;03m# static quantization - get scale and zero point from layer\u001b[39;00m\n\u001b[32m    380\u001b[39m     scale = \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_scale\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/compressed_tensors/quantization/utils/helpers.py:201\u001b[39m, in \u001b[36mcompute_dynamic_scales_and_zp\u001b[39m\u001b[34m(value, args, module, global_scale)\u001b[39m\n\u001b[32m    198\u001b[39m     min_val = torch.amin(value, dim=reduce_dims, keepdims=keep_dims)\n\u001b[32m    199\u001b[39m     max_val = torch.amax(value, dim=reduce_dims, keepdims=keep_dims)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m calculate_qparams(min_val, max_val, args, global_scale=global_scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/compressed_tensors/quantization/utils/helpers.py:93\u001b[39m, in \u001b[36mcalculate_qparams\u001b[39m\u001b[34m(min_vals, max_vals, quantization_args, global_scale)\u001b[39m\n\u001b[32m     89\u001b[39m max_vals = torch.max(max_vals, torch.zeros_like(max_vals))\n\u001b[32m     91\u001b[39m device = min_vals.device\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m bit_min, bit_max = calculate_range(quantization_args, device)\n\u001b[32m     94\u001b[39m bit_range = bit_max - bit_min\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_fp4(quantization_args=quantization_args):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/MCGLH/lib/python3.11/site-packages/compressed_tensors/quantization/utils/helpers.py:218\u001b[39m, in \u001b[36mcalculate_range\u001b[39m\u001b[34m(quantization_args, device)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m quantization_args.type == QuantizationType.FLOAT:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m quantization_args.num_bits == \u001b[32m8\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         q_max = torch.tensor(FP8_E4M3_DATA.max, device=device)\n\u001b[32m    219\u001b[39m         q_min = torch.tensor(FP8_E4M3_DATA.min, device=device)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m quantization_args.num_bits == \u001b[32m4\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Dataset caching\n",
    "from copy import deepcopy\n",
    "\n",
    "cached = deepcopy(dataset)\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "for i, x in enumerate(cached['train']):\n",
    "    print(f'train_{i}')\n",
    "    print(x['instruction'])\n",
    "    tokens = teacher_tokenizer(bielik_prompt(x['instruction']), return_tensors='pt').to(device)\n",
    "    output = teacher_tokenizer.decode(teacher.generate(**tokens, max_new_tokens=512)[0], skip_special_tokens=True)\n",
    "    cached['train'][i]['output'] = output\n",
    "\n",
    "print(cached['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74eb169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, 1/184: Training on batch...\n",
      "['user\\nOpisz, jak działa telefon komórkowy.\\n\\nassistant\\nTelefon komórkowy działa poprzez przesyłanie sygnałów radiowych między urządzeniem a stacjami bazowymi, co umożliwia komunikację.', 'user\\nDziękuję za Twój czas.\\n\\nassistant\\nCała przyjemność po mojej stronie.']\n",
      "loss= 9.36070728302002\n",
      "\n",
      "Epoch 0, 2/184: Training on batch...\n",
      "['user\\nDo widzenia!\\n\\nassistant\\nDo zobaczenia wkrótce!', 'user\\nCzym jest magnetyzm?\\n\\nassistant\\nMagnetyzm to zjawisko fizyczne, w którym materia wywiera siły przyciągania lub odpychania na inne materiały.']\n",
      "loss= 8.052878379821777\n",
      "\n",
      "Epoch 0, 3/184: Training on batch...\n",
      "['user\\nCzym jest wolność słowa?\\n\\nassistant\\nWolność słowa to prawo do wyrażania opinii i idei bez cenzury lub strachu przed represjami.', 'user\\nMiłego dnia!\\n\\nassistant\\nWzajemnie, życzę miłego dnia!']\n",
      "loss= 8.073185920715332\n",
      "\n",
      "Epoch 0, 4/184: Training on batch...\n",
      "['user\\nNapisz krótkie pytanie o pogodę.\\n\\nassistant\\nJaka jest prognoza pogody na weekend?', 'user\\nCo to jest GPT-3?\\n\\nassistant\\nGPT-3 to duży model językowy stworzony przez OpenAI, zdolny do generowania tekstu zbliżonego do ludzkiego.']\n",
      "loss= 7.706363201141357\n",
      "\n",
      "Epoch 0, 5/184: Training on batch...\n",
      "['user\\nNapisz krótkie życzenia urodzinowe.\\n\\nassistant\\nWszystkiego najlepszego z okazji urodzin! Spełnienia marzeń i samych radosnych chwil!', 'user\\nJakie są ograniczenia AI?\\n\\nassistant\\nOgraniczenia obejmują brak zdrowego rozsądku, kreatywności w ludzkim sensie i zdolności do prawdziwego zrozumienia.']\n",
      "loss= 7.549710273742676\n",
      "\n",
      "Epoch 0, 6/184: Training on batch...\n",
      "['user\\nOpisz, jak powstaje burza piaskowa.\\n\\nassistant\\nBurza piaskowa powstaje, gdy silny wiatr unosi duże ilości piasku i pyłu z suchych obszarów, tworząc gęstą chmurę pyłu.', 'user\\nCo to jest kwant?\\n\\nassistant\\nKwant to najmniejsza, niepodzielna porcja energii lub innej wielkości fizycznej, zgodnie z teorią kwantową.']\n",
      "loss= 8.274480819702148\n",
      "\n",
      "Epoch 0, 7/184: Training on batch...\n",
      "['user\\nCzym jest termodynamika?\\n\\nassistant\\nTermodynamika to gałąź fizyki zajmująca się relacjami między ciepłem a innymi formami energii.', 'user\\nWytłumacz, czym jest grawitacja.\\n\\nassistant\\nGrawitacja to fundamentalna siła, która przyciąga do siebie wszystkie obiekty posiadające masę, odpowiadając za ruch planet i utrzymywanie się atmosfery.']\n",
      "loss= 8.150145530700684\n",
      "\n",
      "Epoch 0, 8/184: Training on batch...\n",
      "['user\\nCo dalej?\\n\\nassistant\\nTeraz możemy przejść do [następny krok].', 'user\\nWymień trzy gatunki muzyczne.\\n\\nassistant\\nTrzy gatunki muzyczne to rock, pop i jazz.']\n",
      "loss= 6.4859161376953125\n",
      "\n",
      "Epoch 0, 9/184: Training on batch...\n",
      "['user\\nWytłumacz, czym jest efekt aureoli.\\n\\nassistant\\nEfekt aureoli to błąd poznawczy, w którym ogólne pozytywne wrażenie o osobie lub rzeczy wpływa na postrzeganie jej konkretnych cech.', 'user\\nDzień dobry, proszę o informacje.\\n\\nassistant\\nDzień dobry, jakiej informacji potrzebujesz?']\n",
      "loss= 8.341477394104004\n",
      "\n",
      "Epoch 0, 10/184: Training on batch...\n",
      "['user\\nJak AI wpływa na cyberbezpieczeństwo?\\n\\nassistant\\nAI wspiera wykrywanie zagrożeń, analizę luk w zabezpieczeniach i obronę przed atakami.', 'user\\nNapisz krótkie pozdrowienie.\\n\\nassistant\\nCześć! Mam nadzieję, że u Ciebie wszystko dobrze. Pozdrowienia!']\n",
      "loss= 6.777453422546387\n",
      "\n",
      "Epoch 0, 11/184: Training on batch...\n",
      "['user\\nCzy AI może popełniać błędy?\\n\\nassistant\\nTak, AI może popełniać błędy, szczególnie gdy dane wejściowe są niepełne lub błędne.', 'user\\nCo to jest grawitacja?\\n\\nassistant\\nGrawitacja to fundamentalna siła przyciągania między ciałami posiadającymi masę.']\n",
      "loss= 6.7773518562316895\n",
      "\n",
      "Epoch 0, 12/184: Training on batch...\n",
      "['user\\nPodaj definicję innowacji.\\n\\nassistant\\nInnowacja to wprowadzenie nowych idei, metod, produktów lub procesów, które przynoszą znaczącą wartość lub ulepszenie.', 'user\\nOpisz, jak działa zegar.\\n\\nassistant\\nZegar działa dzięki mechanizmowi, który regularnie odmierza upływ czasu, często za pomocą wahadła lub kryształu kwarcu.']\n",
      "loss= 7.36462926864624\n",
      "\n",
      "Epoch 0, 13/184: Training on batch...\n",
      "['user\\nCzym jest prawda?\\n\\nassistant\\nPrawda to zgodność myśli lub twierdzenia z rzeczywistością lub faktem.', 'user\\nWytłumacz, czym jest efekt cieplarniany.\\n\\nassistant\\nEfekt cieplarniany to proces, w którym gazy w atmosferze Ziemi zatrzymują ciepło, co powoduje wzrost temperatury planety.']\n",
      "loss= 6.963526248931885\n",
      "\n",
      "Epoch 0, 14/184: Training on batch...\n",
      "['user\\nWyjaśnij, czym jest uczenie maszynowe.\\n\\nassistant\\nUczenie maszynowe to poddziedzina AI, która pozwala systemom uczyć się na podstawie danych, bez wyraźnego programowania.', 'user\\nJakie są wyzwania w rozwoju AI?\\n\\nassistant\\nWyzwania to m.in. brak dużych zbiorów danych, interpretowalność modeli i bezpieczeństwo.']\n",
      "loss= 6.8701863288879395\n",
      "\n",
      "Epoch 0, 15/184: Training on batch...\n",
      "['user\\nNapisz krótką historię o odważnym rycerzu.\\n\\nassistant\\nOdważny rycerz stanął do walki ze smokiem. Po zaciętym boju pokonał bestię, ratując królestwo przed zagładą.', 'user\\nPodaj trzy przykłady krajów z monarchią.\\n\\nassistant\\nTrzy kraje z monarchią to Wielka Brytania, Japonia i Arabia Saudyjska.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Training on batch...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mloss= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_step(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m99\u001b[39m:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSaving model...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     33\u001b[39m optimizer.step()\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# print(torch.cuda.memory_summary())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 256\n",
    "gc.collect()\n",
    "dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=2, shuffle=True)\n",
    "teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "\n",
    "student = AutoModelForCausalLM.from_pretrained('sdadas/polish-gpt2-small').to(device)\n",
    "teacher.eval()\n",
    "student.train()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(student.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_step(batch):\n",
    "    gc.collect()\n",
    "        \n",
    "    prompt = batch_to_prompt(batch)\n",
    "    print(prompt)\n",
    "    # print(torch.cuda.memory_summary())\n",
    "    # TODO: one tokenization + mask prompt\n",
    "    student_inputs = student_tokenizer(batch['instruction'], return_tensors='pt', padding=\"max_length\", truncation=True, max_length=MAX_TOKENS).to(device)\n",
    "    student_labels = student_tokenizer(batch['output'], return_tensors='pt', padding=\"max_length\", truncation=True, max_length=MAX_TOKENS).to(device)\n",
    "    student_outputs = student(**student_inputs, labels=student_labels['input_ids'], max_length=MAX_TOKENS)\n",
    "\n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "    loss = student_outputs.loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # print(torch.cuda.memory_summary())\n",
    "    optimizer.step()\n",
    "    \n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print()\n",
    "        print(f'Epoch {epoch}, {i+1}/{len(dataloader)}: Training on batch...')\n",
    "        print(f'loss= {train_step(batch)}')\n",
    "        if i % 100 == 99:\n",
    "            print('Saving model...')\n",
    "            student.save_pretrained(f'ptaqqqq/polish-gpt2-small-distilled-{epoch}-{i}')\n",
    "            student_tokenizer.save_pretrained(f'ptaqqqq/polish-gpt2-small-distilled-{epoch}-{i}')\n",
    "\n",
    "    print('Saving model...')\n",
    "    student.save_pretrained(f'ptaqqqq/polish-gpt2-small-distilled-{epoch}')\n",
    "    student_tokenizer.save_pretrained(f'ptaqqqq/polish-gpt2-small-distilled-{epoch}')\n",
    "\n",
    "print('Training finished.')\n",
    "print('Saving final model...')\n",
    "student.save_pretrained('ptaqqqq/polish-gpt2-small-distilled-final')\n",
    "student_tokenizer.save_pretrained('ptaqqqq/polish-gpt2-small-distilled-final')  \n",
    "print('Testing final model:')\n",
    "test_model(student, student_tokenizer, custom_prompt=dataset['train'][0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac49090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   89,  1446,   203, 49900, 15309, 12843,   282,  2834,   264, 23796,\n",
      "           203,   400,   393,  1111,    88,   203]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([1, 528])\n",
      "user\n",
      "Napisz krótką prośbę o pomoc w wyborze\n",
      "assistant\n",
      "assistantatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedatedated\n"
     ]
    }
   ],
   "source": [
    "test_model(student, student_tokenizer, custom_prompt=\"user\\nNapisz krótką prośbę o pomoc w wyborze\\nassistant\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f17ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved models\n",
    "import os\n",
    "saved_models = [f for f in os.listdir('ptaqqqq/') if f.startswith('polish-gpt2-small-distilled-')]\n",
    "print('Saved models:')\n",
    "for model in saved_models:\n",
    "    print(model)\n",
    "\n",
    "if False:\n",
    "    for name, module in student.named_modules():\n",
    "        if \"c_attn\" in name or \"c_proj\" in name:\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d1322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCGLH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
